<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Integration Startup Presentation</title>
    <!-- Include reveal.js CSS from CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/theme/black.min.css">
    <style>
        .reveal h1,
        .reveal h2,
        .reveal p {
            color: #fff;
        }

        .reveal ul {
            list-style-type: disc;
            margin-left: 20px;
        }

        .video-container {
            position: relative;
            padding-bottom: 56.25%;
            /* 16:9 aspect ratio */
            height: 0;
            overflow: hidden;
        }

        .video-container iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
    </style>
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <!-- Slide 1: Introduction -->
            <section
                data-narration="Welcome to our AI Integration Startup. We're here to revolutionize how humans and artificial intelligence interact by developing intuitive interfaces and seamless interaction methods. For instance, imagine an AI companion that understands your needs as effortlessly as a close friend. Our mission is to make this a reality, while our vision looks toward a future where AI doesn’t just assist but truly enhances human creativity, productivity, and everyday experiences—blending seamlessly into our lives.">
                <h1>AI Integration Startup</h1>
                <p><strong>Mission:</strong> Enhancing human-AI integration through intuitive interfaces and interaction
                    methods.</p>
                <p><strong>Vision:</strong> A future where AI seamlessly enhances human capabilities and experiences.
                </p>
            </section>

            <!-- Slide 2: Project 1 - Human-LLM Interaction -->
            <section
                data-narration="Project 1 is about reimagining research with AI as a partner, not a solo act. We’re building a system where Large Language Models dig into research topics—like exploring rare disease trends—but in a way that’s interactive and collaborative. Imagine a researcher asking, ‘What’s behind this spike in cases?’ and the AI not only pulls data but sparks a back-and-forth, suggesting angles and refining ideas together. We’re designing these collaboration patterns—think real-time chats or shared brainstorming spaces—so humans and AI join forces, blending intuition with raw computational power to uncover insights neither could alone.">
                <h2>Project 1: Interactive Human-LLM Research Collaboration</h2>
                <ul>
                    <li>Developing AI to investigate research topics interactively using Large Language Models (LLMs)
                    </li>
                    <li>Focus: Designing collaboration patterns for joint human-AI exploration</li>
                    <li>Example: Real-time research dialogue to analyze medical trends or scientific questions</li>
                </ul>
            </section>

            <!-- Slide 3: Video Example - ChatPiano Interaction -->
            <section
                data-narration="Check this out: our ChatPiano project in action! This isn’t just a demo—it’s a window into the interactive magic we’re crafting. Watch how we’ve designed a piano that chats back, fusing AI with creativity in a way that feels alive and personal. It’s a stepping stone for us—taking interaction design from research to artistry. We’re building pipelines for music creation and sparking casual insights, envisioning an AI that doesn’t just assist but inspires your next melody or breakthrough idea!"
                data-no-auto-advance="true">
                <p>Explore our ChatPiano interaction design:</p>
                <div class="video-container">
                    <iframe id="bilibili-video"
                        src="https://player.bilibili.com/player.html?bvid=BV1qP6DYyEcP&autoplay=0" frameborder="0"
                        allowfullscreen></iframe>
                </div>
            </section>
            <!-- Slide 3: Project 2 - Human-Robot Interaction -->
            <section
                data-narration="Project 2 is all about designing how humans and robots work together, using Reinforcement Learning to make it happen. Our goal is to build trust and understanding in shared spaces—like a factory floor where a robot doesn’t just follow orders but communicates its actions. Imagine a robot assembling parts, pausing to say, ‘I’ve detected a defect here,’ and adjusting based on the human operator’s feedback. This collaboration makes work safer, more efficient, and even more enjoyable.">
                <h2>Project 2: Human-Robot Interaction Design</h2>
                <ul>
                    <li>Designing interaction methods for robots using Reinforcement Learning (RL)</li>
                    <li>Goal: Improve understanding and trust in collaborative settings</li>
                    <li>Example: Manufacturing robot communicating tasks to human operators</li>
                </ul>
            </section>

            <!-- Slide 4: Video Example (Updated) -->
            <section
                data-narration="Take a look at this: Apple’s Pixar-inspired robot lamp in action. This isn’t just a cool design—it’s a glimpse into what intuitive robot interaction can be. The lamp moves with personality, responding to its environment in a way that feels almost human. Our project builds on this idea, pushing for robots that don’t just perform tasks but actively engage with us, using smart design to make collaboration feel natural and intuitive. Watch how this little lamp brings technology to life!"
                data-no-auto-advance="true">
                <p>Watch Apple’s Pixar-inspired robot lamp in action:</p>
                <div class="video-container">
                    <iframe id="youtube-video" src="https://www.youtube.com/embed/iJtTfIj1Dd8?enablejsapi=1"
                        frameborder="0" allowfullscreen></iframe>
                </div>
            </section>

            <!-- Slide 5: Methodology -->
            <section
                data-narration="Our approach is all about getting to the core of what works. We lean on human-centered design—putting real people first by gathering their feedback, testing prototypes, and refining relentlessly. Picture tweaking an AI tool until it’s just right for a researcher’s workflow. We also draw from Elon Musk’s First Principles, stripping problems down to their basics—like asking, ‘What’s the simplest way a robot can understand us?’—then building fresh solutions from there. Add in collaboration with experts—engineers, psychologists, you name it—and we’re crafting tech that’s not just new, but truly relevant.">
                <h2>Methodology</h2>
                <ul>
                    <li>Human-centered design approach</li>
                    <li>Prioritizing user feedback and iterative testing</li>
                    <li>Incorporating Elon Musk’s First Principles: Breaking down problems to fundamental truths</li>
                    <li>Collaboration with domain experts for relevance</li>
                </ul>
            </section>

            <!-- Slide 6: Future Directions -->
            <section
                data-narration="Looking ahead, we’re dreaming big. We want to expand our LLM tools into fields like psychology—helping therapists decode patient emotions—or education, tailoring lessons to each student’s pace. For robots, we’re exploring multimodal communication—think a robot that speaks, gestures, and even shows visuals to get its point across. Our ultimate aim is synergy: a world where humans and AI don’t just coexist but amplify each other’s strengths in ways we’re only beginning to imagine.">
                <h2>Future Directions</h2>
                <ul>
                    <li>Expanding LLM applications to fields like psychology and education</li>
                    <li>Integrating multimodal communication for robots (visual, auditory)</li>
                    <li>Aiming for synergy between human and AI capabilities</li>
                </ul>
            </section>

            <!-- Slide 7: Team -->
            <section
                data-narration="Meet the heartbeat of our mission—our team. For over three years, we’ve been crafting this vision, originally under the AIERLab Society banner. Curious for more? Check out aierlab.tech! We’re a global crew—hailing from the US, UK, China, Japan, and the UAE—blending undergrads, postgrads, and seasoned pros. Our passion? Building AI that connects, like our pet robot—a companion that’s more than tech, it’s a friend. From privacy-preserving AI to large language model services, and even deploying a pet robot that responds to your voice, we’ve tackled it all. Our diverse backgrounds fuel creativity, turning challenges into breakthroughs, one project at a time.">
                <h2>Our Team</h2>
                <ul>
                    <li>Over 3 years of innovation with AIERLab Society (<a href="https://aierlab.tech"
                            target="_blank">aierlab.tech</a>)</li>
                    <li>Diverse global team: Undergraduates, postgraduates, and professionals from the US, UK, China,
                        Japan, and UAE</li>
                    <li>Expertise in AI infrastructure, robotics, and deployment</li>
                    <li>Projects: Privacy-preserving AI, LLM services, and our interactive pet robot</li>
                </ul>
            </section>

            <!-- Slide 8: Contact and Conclusion -->
            <section
                data-narration="That wraps up our story—for now. We’re excited to connect with you, whether you’re curious about our work, have ideas to share, or want to join us on this journey. Drop us a line at info@aiintegration.com—we’d love to chat. Thanks for tuning in and exploring how we’re bridging the gap between humans and AI. Here’s to a future where technology feels like a partner, not just a tool!">
                <h2>Contact and Conclusion</h2>
                <p>For further engagement, reach us at: <a href="mailto:hobart.yang@qq.com">hobart.yang@qq.com</a></p>
                <p>Thank you for your interest!</p>
            </section>
        </div>
    </div>

    <!-- Include reveal.js JavaScript from CDN -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.3.1/reveal.min.js"></script>
    <!-- Include YouTube IFrame API -->
    <script src="https://www.youtube.com/iframe_api"></script>
    <script>
        // Initialize reveal.js
        Reveal.initialize({
            controls: true,
            progress: true,
            history: true,
            center: true,
            transition: 'slide',
            autoSlide: 0,
            loop: false,
            autoSlideStoppable: true
        });

        const synth = window.speechSynthesis;
        let currentUtterance = null;
        let micStream = null;
        let audioContext = null;
        let micAudio = null;
        let player;

        // YouTube Player Initialization
        function onYouTubeIframeAPIReady() {
            player = new YT.Player('youtube-video', {
                events: {
                    'onReady': onPlayerReady,
                    'onStateChange': onPlayerStateChange
                }
            });
        }

        function onPlayerReady(event) {
            // Player is ready; no action needed here yet
        }

        function onPlayerStateChange(event) {
            if (event.data === YT.PlayerState.ENDED) {
                Reveal.next(); // Advance to next slide when video ends
            }
        }

        // Check if narration is working
        function isNarrationWorking() {
            const testUtterance = new SpeechSynthesisUtterance('');
            synth.speak(testUtterance);
            return synth.speaking;
        }

        // Narrate with SpeechSynthesis, respecting data-no-auto-advance
        function narrateWithSpeechSynthesis(slide) {
            const narrationText = slide.getAttribute('data-narration');
            if (!narrationText) return;

            if (currentUtterance) {
                synth.cancel();
            }

            currentUtterance = new SpeechSynthesisUtterance(narrationText);
            currentUtterance.lang = 'en-US';
            currentUtterance.rate = 1.0;
            currentUtterance.volume = 1.0;

            synth.speak(currentUtterance);

            if (!slide.hasAttribute('data-no-auto-advance')) {
                currentUtterance.onend = () => {
                    Reveal.next();
                };
            }
        }

        // Narrate with Microphone as fallback, respecting data-no-auto-advance
        async function narrateWithMicrophone(slide) {
            const narrationText = slide.getAttribute('data-narration');
            if (!narrationText) return;

            try {
                if (micStream) {
                    micStream.getTracks().forEach(track => track.stop());
                }
                if (audioContext) {
                    audioContext.close();
                }

                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(micStream);
                micAudio = new Audio();
                micAudio.srcObject = micStream;

                const analyser = audioContext.createAnalyser();
                source.connect(analyser);
                analyser.fftSize = 2048;
                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);

                micAudio.play();

                let silenceStart = null;
                const checkSilence = () => {
                    analyser.getByteTimeDomainData(dataArray);
                    const maxAmplitude = Math.max(...dataArray) / 128 - 1;
                    const threshold = 0.05;

                    if (maxAmplitude < threshold) {
                        if (!silenceStart) silenceStart = Date.now();
                        if (Date.now() - silenceStart > 2000 && !slide.hasAttribute('data-no-auto-advance')) {
                            stopMicrophone();
                            Reveal.next();
                        }
                    } else {
                        silenceStart = null;
                    }

                    if (micStream.active) {
                        requestAnimationFrame(checkSilence);
                    }
                };
                checkSilence();
            } catch (error) {
                console.error('Microphone access failed:', error);
                Reveal.next();
            }
        }

        // Stop microphone narration
        function stopMicrophone() {
            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
                micStream = null;
            }
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            if (micAudio) {
                micAudio.pause();
                micAudio = null;
            }
        }

        // Main narration function with fallback
        function narrateSlide(slide) {
            if (synth && isNarrationWorking()) {
                stopMicrophone();
                narrateWithSpeechSynthesis(slide);
            } else {
                console.warn('SpeechSynthesis not working, falling back to microphone');
                narrateWithMicrophone(slide);
            }
        }

        // Handle slide changes
        Reveal.addEventListener('slidechanged', (event) => {
            if (currentUtterance) {
                synth.cancel();
            }
            stopMicrophone();
            narrateSlide(event.currentSlide);

            // Pause video if leaving the video slide
            if (event.previousSlide && event.previousSlide.querySelector('#youtube-video')) {
                if (player && player.pauseVideo) {
                    player.pauseVideo();
                }
            }

            // Play video if entering the video slide
            if (event.currentSlide.querySelector('#youtube-video')) {
                if (player && player.playVideo) {
                    player.playVideo();
                }
            }
        });

        // Start narration for the first slide
        narrateSlide(Reveal.getCurrentSlide());
    </script>
</body>

</html>
